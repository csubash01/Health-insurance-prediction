import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv("insurance.csv")


df.head()


df.tail(5)


df.info()


df.shape


pd.set_option("display.float_format","{:.2f}".format)


df


sns.set(style = "whitegrid", palette = "Set2",font_scale = 1.1)


df.head()


df.duplicated().sum()


df.isna().sum()


df.isna().sum().sum()


df.dropna(inplace = True)


df.shape


df.describe(include="all")


numeric_cols = ["age","bmi","bloodpressure","children","claim"]
df[numeric_cols].hist(bins = 20,figsize = (12,8),color = "skyblue",edgecolor = "black")
plt.suptitle("Distribution of Numerical features", fontsize = 16)
plt.show()


cat_cols=["gender","diabetic","smoker","region"]
plt.figure(figsize=(12,8))

for i, col in enumerate(cat_cols,1):
    plt.subplot(2,2,i)
    sns.countplot(data= df,x=col)
    plt.title(f"Distribution of {col}")

plt.tight_layout()
plt.show()


df.groupby(["gender","smoker"])["claim"].mean().round(2)


plt.figure(figsize=(8,5))
sns.barplot(data = df,x = "gender", y= "claim", hue = "smoker", estimator = "mean" , errorbar="sd")
plt.title("Average Insurance Claim by Gender & Smoking Status")
plt.show()


pivot_region_diabetic = df.groupby(["region", "diabetic",]) ["claim"].mean().unstack()


pivot_region_diabetic


pivot_region_diabetic.plot(kind='bar',figsize=(8,5))
plt.title("Average Claim by Region & Diabetic Status")
plt.ylabel("Mean Claim")
plt.show()


pivot_table = pd.pivot_table(df, values="claim", index = "region",columns = "smoker", aggfunc="mean")
pivot_table


pivot_table = pd.pivot_table(df, values="claim", index = "children",columns = "diabetic", aggfunc="mean")
pivot_table


numeric_cols


plt.figure(figsize=(8,6))
sns.heatmap(df[numeric_cols].corr(), annot = True, cmap = "coolwarm", fmt =".2f")
plt.title("correlation Heatmap")
plt.show()


sns.scatterplot(data=df, x ="age", y = "claim", hue = "smoker", style  = "gender", alpha = 0.7)
plt.title("Claim Vs Age by Smoker and Gender")
plt.show()


sns.regplot(data = df, x="bmi", y = "claim", scatter_kws={"alpha":0.6})
plt.title("Relationship between BMI and Claim Amount")
plt.show()


sns.boxplot(data = df, x = "children", y = "claim")
plt.title("Claim distribution by number")
plt.show()


df["age_group"] = pd.cut(df["age"], bins = [0,18,30,45,60,100], labels = ["<18","18-30","31-45","46-60","60+"])


df


df["age_group"].value_counts()


sns.barplot(data = df, x= "age_group", y = "claim", estimator = "mean", errorbar = "sd")
plt.title("Average Claim by Age Group")
plt.show()


df["bmi_category"] = pd.cut(df["bmi"], bins = [0,18.5,24.9,29.9,100], labels =["Underweight","Normal","Overweight","Obese"])


df["bmi_category"].value_counts()


sns.boxplot(data = df, x="bmi_category", y="claim",hue = "smoker")
plt.title("Claim Distribution by BMI Category and Smoking Status")
plt.show()


import warnings
warnings.filterwarnings("ignore")


sns.boxplot(data = df, x="bmi_category", y="claim")
plt.title("Claim Distribution by BMI Category")
plt.show()





region_stats = df.groupby("region").agg(
    smoker_rate = ("smoker", lambda x: (x=="Yes").mean() *100 ),
    mean_claim = ("claim","mean")
).reset_index()


region_stats


fig, ax1 = plt.subplots(figsize= (8,5))
sns.barplot(data =region_stats, x ="region", y = "smoker_rate", ax = ax1, alpha = 0.6)
ax2 = ax1.twinx()
sns.lineplot(data = region_stats, x= "region", y = "mean_claim", ax = ax2, color = "red", marker = "o")

ax1.set_ylabel("Smoker Rate (%)")
ax2.set_ylabel("Average Claim ($)")
plt.title("Smoker Rate and Average Claim by region")
plt.show()


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib


df.columns


X = df[["age","gender","bmi","bloodpressure","diabetic","children","smoker"]]
y = df["claim"]


X


cat_cols = ["gender","diabetic","smoker"]
label_encoders = {}


for col in cat_cols:
    le  = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders [col] = le

    joblib.dump(le,f"label_encoder_{col}.pkl")


X


label_encoders


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)


num_cols = ["age","bmi","bloodpressure", "children"]
scaler = StandardScaler()


X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])


joblib.dump(scaler,"scaler.pkl")


print(X_train.shape, y_train.shape)


print(X_test.shape, y_test.shape)





get_ipython().getoutput("pip install xgboost")


import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor


X_train['age'] = X_train['age'].fillna(X_train['age'].mean())
X_test['age'] = X_test['age'].fillna(X_test['age'].mean())


import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import pandas as pd

# 1. Make sure X_train/X_test have no NaNs
# X_train = X_train.fillna(0)
# X_test = X_test.fillna(0)
# y_train = y_train.fillna(0)
# y_test = y_test.fillna(0)

# 2. Define results dictionary
results = {}

# 3. Define evaluation function (correct name!)
def evaluate_model(model, X_train, X_test, y_train, y_test):
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test,y_pred)
    mae = mean_absolute_error(y_test,y_pred)
    rmse = np.sqrt(mean_squared_error(y_test,y_pred))
    return {"R2": r2, "MAE": mae, "RMSE": rmse}

# 4. Train model
lr = LinearRegression()
lr.fit(X_train, y_train)

# 5. Evaluate and store results
results["Linear Regression"] = evaluate_model(lr, X_train, X_test, y_train, y_test)

print(results)



best_poly_model = None
best_poly_score = -np.inf

for degree in [2,3]:
    poly = PolynomialFeatures(degree = degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    poly_lr  = LinearRegression()
    poly_lr.fit(X_train_poly, y_train)

    score = poly_lr.score(X_test_poly,y_test)

    if score > best_poly_score:

        best_poly_score = score
        best_poly_model = (degree,poly,poly_lr)

degree, poly , poly_lr = best_poly_model

results[f"Polynomial Regression (deg = {degree})"] = evaluate_model(poly_lr, poly.fit_transform(X_train), poly.transform(X_test), y_train, y_test)

print("Polynomial Regression model are trained.")


rf = RandomForestRegressor()

rf_params = {
    "n_estimators": [100,200],
    "max_depth": [None,10,20],
    "min_samples_split": [2,5],
    "min_samples_leaf":[1,2]
}

rf_grid = GridSearchCV(rf, rf_params, cv = 3, scoring = "r2", n_jobs = -1, verbose = 0)
rf_grid.fit(X_train,y_train)
best_rf= rf_grid.best_estimator_

results["Random forest"] = evaluate_model(best_rf, X_train, X_test, y_train, y_test)

print("Random Forest training is completed, best parameters",rf_grid.best_params_)



svr = SVR()

svr_params = {
    "kernel" : ["rbf","poly","linear"],
    "C": [1,10,50],
    "epsilon": [0.1,0.2,0.5],
    "degree": [2,3]
}

svr_grid = GridSearchCV(svr,svr_params,cv = 3, scoring = "r2", n_jobs = -1, verbose = 0)
svr_grid.fit(X_train,y_train)

best_svr = svr_grid.best_estimator_

results["SVR"] = evaluate_model(best_svr,X_train, X_test, y_train,y_test)

print("SVR training is completed, best parameters:" , svr_grid.best_params_)


xgb = XGBRegressor(objective = "reg:squarederror")

xgb_params = {
    "n_estimators":[100,200],
    "max_depth": [3,5,7],
    "learning_rate": [0.01,0.05,0.1],
    "subsample": [0.8,1.0]
}

xgb_grid = GridSearchCV(xgb, xgb_params, cv= 3, scoring="r2", n_jobs = -1, verbose = 0)
xgb_grid.fit(X_train, y_train)
best_xgb = xgb_grid.best_estimator_

results["XGBoost"] = evaluate_model(best_xgb, X_train, X_test, y_train,y_test)

print("XGBoost training is completed, best parameters:", xgb_grid.best_params_)



results


results_df = pd.DataFrame(results).T.sort_values(by = "R2", ascending =False)
results_df


best_rf


models = {
    "Linear Regression": lr,
    "Polynomial Regression":poly_lr,
    "Random Forest": best_rf,
    "XGBoost": best_xgb,
    "SVR": best_svr,
}


best_r2 = results_df["R2"].max()


best_r2


top_model  = results_df[results_df["R2"] == best_r2]


top_model


best_model = models[top_model.index[0]]


best_model


joblib.dump(best_model, "best_model.pkl")
print(f"Best model selected:{top_model.index[0]}")



